---
title: "Data Science Capstone Milestone Report"
author: "Georg Vogelhuber"
date: "17.7.2015"
output: html_document
---

# Summary

The aim of this project is to target the "Next Word Prediciton" problem (NWP), i. e. to predict the next word in a sentence. The final result of this project will be an algorithm for NWP implemented as shiny app. The NWP feature is commonly found in web search sites or virtual keyboards for mobile applications.

For this project we use mainily the `quanteda`, `tm` and `stringi` packages for dealing with text data and tokenizing inputs.

Our model for the NWP will be based on common n-gram techniques:
http://cs.stanford.edu/people/eroberts/courses/soco/projects/2004-05/nlp/techniques_word.html

This milestone report

- describes the available data for this project,
- provides exploratory data analysis and
- gives an outline for the next steps in this project.


# Data acquisition

The data for this project is provided on the course website: https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip. 
The original source of this data is the HC Corpora http://www.corpora.heliohost.org. 
For this project, we are using the English files. The files have been language filtered but may still contain some foreign text.

```{r read_data, echo=FALSE, message=FALSE}
load("news.RData")
load("twitter.RData")
load("blogs.RData")

library(quanteda)
library(ggplot2)
library(dplyr)
library(stringi)
library(pander)
```

For the provided english Corpora we have the following summary statistics:

```{r basic_stats, echo=FALSE, message=FALSE, results="asis"}
rbind(stringi::stri_stats_general(twitter), 
      stringi::stri_stats_general(news), 
      stringi::stri_stats_general(blogs)) %>% as.data.frame -> stri_res
rownames(stri_res) = c("Twitter", "News", "Blogs")
stri_res <- cbind(stri_res, 
                  File = c("en_US.twitter.txt", "en_US.news.txt", "en_US.blogs.txt"),
                  Words = c(sum(stringi::stri_count_words(twitter)),
                            sum(stringi::stri_count_words(news)),
                            sum(stringi::stri_count_words(blogs))),
                  Object.Size = c(object.size(twitter),
                                  object.size(news),
                                  object.size(blogs)),
                  File.Size = file.size("en_US.twitter.txt",
                                        "en_US.news.txt", 
                                        "en_US.blogs.txt"))

stri_res %>% 
  select(File, File.Size, Object.Size, Lines, Words, Chars, -LinesNEmpty, -CharsNWhite) -> stri_res

pandoc.table(stri_res, style="rmarkdown", caption="Basic Statistics for data sources")

```

The table above contains the following data:

- *File*: filename
- *File.Size*: Size of file in bytes
- *Object.Size*: Size of the corresponding object, after reading file into R.
- *Lines*: Number of lines in file
- *Words*: Number of words in file
- *Chars*: Number of characters in file

# Data cleaning & tokenization

The data mentioned above is loaded into R via `stringi::readLines`. Then a corpus is build for
each data set. We take only a random subsample of the provided data for the exploratry data analysis.
The entire raw data is too big to process it in reasonable time.

```{r build_corpus, message=FALSE, warning=FALSE, results=FALSE, error=FALSE}
corpus_news <- corpus(sample(news)[1:1000])
corpus_twitter <- corpus(sample(twitter)[1:1000])
corpus_blogs <- corpus(sample(blogs)[1:1000])
```

Each corpus contains the original data together with some meta info.

```{r clean_raw, echo=FALSE, message=FALSE}
#rm(news)
#rm(twitter)
#rm(blogs)
```

From this corpora we build document feature matrices via the `dfm` method.
This method also performs several cleaning steps:

- convert to lower case
- tokenizing
- remove numbers
- remove punctuation
- remove sepeators
- remove twitter specific characters
- strip additional whitespaces

```{r build_matrix_news}
dfm_news <- dfm(corpus_news)
```

```{r build_matrix_twitter}
dfm_twitter <- dfm(corpus_twitter)
```

```{r build_matrix_blogs}
dfm_blogs <- dfm(corpus_blogs)
```

We keep stopwords and do not use word stemming, as we are not interessed in classifying this text,
but want to predict the next word to be typed. We also keep profane words, as they can help making
predictions for the next word. 

Via the `dfm` function we also create document feature matrices for bi- and tri-grams by specifing the `ngram`-parameter.


# Exploratory Data Analysis

## Top 20 Uni- Bi- and Tri-grams

The following barplots show the frequencies for the 20 most frequent uni- bi- and tri-grams:

```{r plot_uni_gram, echo=FALSE}
top_unis_news <- as.data.frame(topfeatures(dfm_news, 20))
top_unis_twitter <- as.data.frame(topfeatures(dfm_twitter, 20))
top_unis_blogs <- as.data.frame(topfeatures(dfm_blogs, 20))

names(top_unis_news) <- c("Frequency")
names(top_unis_twitter) <- c("Frequency")
names(top_unis_blogs) <- c("Frequency")

top_unis_news$Source = "News"
top_unis_twitter$Source = "Twitter"
top_unis_blogs$Source = "Blogs"

top_unis_news$Unigrams <- rownames(top_unis_news)
top_unis_twitter$Unigrams <- rownames(top_unis_twitter)
top_unis_blogs$Unigrams <- rownames(top_unis_blogs)

top_unis <- rbind(top_unis_news, top_unis_twitter, top_unis_blogs)

ggplot(top_unis, aes(x=Unigrams, y=Frequency, fill=Source)) + 
  geom_bar(stat="identity") +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  ggtitle("Frequencies for most frequent Unigrams") +
  facet_grid(Source~., scales = "free") +
  guides(fill=FALSE)

```

```{r plot_bi_gram, echo=FALSE}
dfm_news_bi <- dfm(corpus_news, ngram=2, verbose =FALSE)
dfm_twitter_bi <- dfm(corpus_twitter, ngram=2, verbose =FALSE)
dfm_blogs_bi <- dfm(corpus_blogs, ngram=2, verbose =FALSE)

top_bi_news <- as.data.frame(topfeatures(dfm_news_bi, 20))
top_bi_twitter <- as.data.frame(topfeatures(dfm_twitter_bi, 20))
top_bi_blogs <- as.data.frame(topfeatures(dfm_blogs_bi, 20))

names(top_bi_news) <- c("Frequency")
names(top_bi_twitter) <- c("Frequency")
names(top_bi_blogs) <- c("Frequency")

top_bi_news$Source = "News"
top_bi_twitter$Source = "Twitter"
top_bi_blogs$Source = "Blogs"

top_bi_news$Bigrams <- rownames(top_bi_news)
top_bi_twitter$Bigrams <- rownames(top_bi_twitter)
top_bi_blogs$Bigrams <- rownames(top_bi_blogs)

top_bi <- rbind(top_bi_news, top_bi_twitter, top_bi_blogs)

ggplot(top_bi, aes(x=Bigrams, y=Frequency, fill=Source)) + 
  geom_bar(stat="identity") +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  ggtitle("Frequencies for most frequent Bigrams") +
  facet_grid(Source~., scales = "free") +
  guides(fill=FALSE)

rm(dfm_news_bi)
rm(dfm_twitter_bi)
rm(dfm_blogs_bi)
```

```{r plot_tri_gram, echo=FALSE}
dfm_news_tri <- dfm(corpus_news, ngram=3, verbose =FALSE)
dfm_twitter_tri <- dfm(corpus_twitter, ngram=3, verbose =FALSE)
dfm_blogs_tri <- dfm(corpus_blogs, ngram=3, verbose =FALSE)

top_tri_news <- as.data.frame(topfeatures(dfm_news_tri, 20))
top_tri_twitter <- as.data.frame(topfeatures(dfm_twitter_tri, 20))
top_tri_blogs <- as.data.frame(topfeatures(dfm_blogs_tri, 20))

names(top_tri_news) <- c("Frequency")
names(top_tri_twitter) <- c("Frequency")
names(top_tri_blogs) <- c("Frequency")

top_tri_news$Source = "News"
top_tri_twitter$Source = "Twitter"
top_tri_blogs$Source = "Blogs"

top_tri_news$Trigrams <- rownames(top_tri_news)
top_tri_twitter$Trigrams <- rownames(top_tri_twitter)
top_tri_blogs$Trigrams <- rownames(top_tri_blogs)

top_tri <- rbind(top_tri_news, top_tri_twitter, top_tri_blogs)

ggplot(top_tri, aes(x=Trigrams, y=Frequency, fill=Source)) + 
  geom_bar(stat="identity") +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  ggtitle("Frequencies for most frequent Trigrams") +
  facet_grid(Source~., scales = "free") +
  guides(fill=FALSE)

rm(dfm_news_tri)
rm(dfm_twitter_tri)
rm(dfm_blogs_tri)
```

## Number of tokens per document

This histograms show the number of tokens per document of the different sources. A document
corresponds to one line in the raw text files.

```{r nr_token, echo=FALSE}

token_data = rbind(data.frame(tokens = ntoken(dfm_news), Source = "News"),
                   data.frame(tokens = ntoken(dfm_twitter), Source = "Twitter"),
                   data.frame(tokens = ntoken(dfm_blogs), Source = "Blogs"))
ggplot(token_data, aes(x=tokens, fill=Source)) + 
  geom_histogram(binwidth=5) +
  theme_bw() +
  facet_grid(Source~.,scales = "free") +
  ggtitle("Number of tokens per document") +
  xlab("Nr. tokens") +
  ylab("Frequency") +
  xlim(0,300)

rm(token_data)
  
```

## Number of sentences per document

This histograms show the number of sentences per document of the different sources. A document
corresponds to one line in the raw text files.

```{r nr_sentence, echo=FALSE}

token_data = rbind(data.frame(tokens = nsentence(corpus_news), Source = "News"),
                   data.frame(tokens = nsentence(corpus_twitter), Source = "Twitter"),
                   data.frame(tokens = nsentence(corpus_blogs), Source = "Blogs"))
ggplot(token_data, aes(x=tokens, fill=Source)) + 
  geom_histogram(binwidth=1) +
  theme_bw() +
  facet_grid(Source~.,scales = "free") +
  ggtitle("Number of sentences per document") +
  xlab("Nr. sentences") +
  ylab("Frequency") +
  xlim(0,20)
  
```


## Number of words vs. corpus covered

The following line plot shows the number of unique words required to cover a certain percentage of the sampled documents.

```{r plot_coverage, echo=FALSE}
words_news <- rev(sort(colSums(dfm_news)))/sum(dfm_news)
words_twitter <- rev(sort(colSums(dfm_twitter)))/sum(dfm_twitter)
words_blogs <- rev(sort(colSums(dfm_blogs)))/sum(dfm_blogs)


words_news <- data.frame(cum_p = cumsum(words_news), 
                         words = seq_along(words_news),
                         Source = "News") %>% filter(words %% 100 == 0)
words_twitter <- data.frame(cum_p = cumsum(words_twitter), 
                            words = seq_along(words_twitter),
                            Source = "Twitter") %>% filter(words %% 100 == 0)
words_blogs <- data.frame(cum_p = cumsum(words_blogs), 
                          words = seq_along(words_blogs),
                          Source = "Blogs") %>% filter(words %% 100 == 0)

rownames(words_news) <- words_news$words
rownames(words_twitter) <- words_twitter$words
rownames(words_blogs) <- words_blogs$words

words_coverage <- rbind(words_news, words_twitter, words_blogs)

rm(words_news)
rm(words_twitter)
rm(words_blogs)

ggplot(words_coverage, aes(x=words, y=cum_p, color=Source)) + 
  geom_line() + 
  xlab("Nr. unique words") + 
  ylab("Percentage of sample covered") + 
  theme_bw() + 
  ggtitle("Number of unique words required to cover percentage of sample data")

rm(words_coverage)
```
  
The three sources are very different in the number of words required to cover a certain percentage.
  
## Lexical complexity

The type-token-ratio is a measure for the lexical complexity of a document. The following boxplot
shows the distribution of the lexical complexity for the documents in our sample.

```{r plot_ttr, echo=FALSE, warning=FALSE}
rm(corpus_news)
rm(corpus_twitter)
rm(corpus_blogs)
rm(news)
rm(blogs)
rm(twitter)

ld_news <- lexdiv(dfm_news, measure = "TTR")
ld_twitter <- lexdiv(dfm_twitter, measure = "TTR")
ld_blogs <- lexdiv(dfm_blogs, measure = "TTR")

ld <- rbind(data.frame(ttr = ld_news, Source = "News"),
            data.frame(ttr = ld_twitter, Source = "Twitter"),
            data.frame(ttr = ld_blogs, Source = "Blogs"))

ggplot(ld, aes(y=ttr, x=Source, color=Source)) + 
  geom_boxplot() +
  theme_bw() +
  xlab("") +
  ylab("Type/Token-Ratio") +
  ggtitle("Type/Token-Ratio by Source")

```

# Next Steps

I plan the following steps to develop and improve the prediciton algorithm:

0. Split data into training, cross-validation and testing set. Build an evaluation function for
   calculating the accuracy of the modell.
1. Build a basic $n$-gram modell using interpolation with $n <= 4$. 
2. Split corpus first into sentences, insert a start sentence tag and then build the $n$-gram modell,
   so the prediciton algorithm takes the beginning of a new sentence into account.
3. Remove documents with fewer than 3 tokens from the modell.
4. Limit the modell to about 10k words and replace all words not in this list by a tag for "Unknown".
   This step should reduce the size of the modell dramatically, so it will possibly run on shiny.
   
5. Use Kneser-Ney-Smoothing to improve the predictive quality of the algorithm.
6. Increase the value of $n$. 
7. Use more data from other sources. Possible sources could be:

   - Wikipedia
   - Get additional tweets
   - Use exisiting corpora from R packages.
   
8. Implement profanity filtering.

